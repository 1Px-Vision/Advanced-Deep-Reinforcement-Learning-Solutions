{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import pygame\n",
        "from IPython import display\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "class OurModel(nn.Module):\n",
        "    def __init__(self, input_shape, action_space, dueling):\n",
        "        super(OurModel, self).__init__()\n",
        "        self.dueling = dueling\n",
        "\n",
        "        self.fc1 = nn.Linear(input_shape[0], 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 64)\n",
        "\n",
        "        if dueling:\n",
        "            self.state_value = nn.Linear(64, 1)\n",
        "            self.action_advantage = nn.Linear(64, action_space)\n",
        "        else:\n",
        "            self.output = nn.Linear(64, action_space)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(m.weight)\n",
        "                m.bias.data.fill_(0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "\n",
        "        if self.dueling:\n",
        "            state_value = self.state_value(x)\n",
        "            action_advantage = self.action_advantage(x)\n",
        "            action_advantage_mean = action_advantage.mean(dim=1, keepdim=True)\n",
        "            q_value = state_value + action_advantage - action_advantage_mean\n",
        "        else:\n",
        "            q_value = self.output(x)\n",
        "\n",
        "        return q_value\n",
        "\n",
        "class D3QN:\n",
        "    def __init__(self, env_name):\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(env_name)\n",
        "        self.env.seed(0)\n",
        "        self.env._max_episode_steps = 4000\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "        self.action_size = self.env.action_space.n\n",
        "\n",
        "        self.EPISODES = 500\n",
        "        memory_size = 10000\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95\n",
        "\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((640, 480))\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.metadata = {\"render_fps\": 30}\n",
        "\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.0005\n",
        "        self.batch_size = 32\n",
        "\n",
        "        self.ddqn = True\n",
        "        self.Soft_Update = False\n",
        "        self.dueling = True\n",
        "        self.epsilot_greedy = False\n",
        "\n",
        "        self.TAU = 0.1\n",
        "\n",
        "        self.Save_Path = 'Models'\n",
        "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
        "        self.scores, self.episodes, self.average = [], [], []\n",
        "\n",
        "        self.Model_name = os.path.join(self.Save_Path, self.env_name + \"_e_greedy.pth\")\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = OurModel(input_shape=(self.state_size,), action_space=self.action_size, dueling=self.dueling).to(self.device)\n",
        "        self.target_model = OurModel(input_shape=(self.state_size,), action_space=self.action_size, dueling=self.dueling).to(self.device)\n",
        "        self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.00025, alpha=0.95, eps=0.01)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state, decay_step):\n",
        "        if self.epsilot_greedy:\n",
        "            explore_probability = self.epsilon_min + (self.epsilon - self.epsilon_min) * np.exp(-self.epsilon_decay * decay_step)\n",
        "        else:\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= (1 - self.epsilon_decay)\n",
        "            explore_probability = self.epsilon\n",
        "\n",
        "        if explore_probability > np.random.rand():\n",
        "            return random.randrange(self.action_size), explore_probability\n",
        "        else:\n",
        "            state = torch.FloatTensor(state).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                action_values = self.model(state)\n",
        "            return torch.argmax(action_values).item(), explore_probability\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        state = torch.FloatTensor(np.vstack([e[0] for e in minibatch])).to(self.device)\n",
        "        action = torch.LongTensor(np.array([e[1] for e in minibatch])).to(self.device)\n",
        "        reward = torch.FloatTensor(np.array([e[2] for e in minibatch])).to(self.device)\n",
        "        next_state = torch.FloatTensor(np.vstack([e[3] for e in minibatch])).to(self.device)\n",
        "        done = torch.FloatTensor(np.array([e[4] for e in minibatch])).to(self.device)\n",
        "\n",
        "        target = self.model(state).to(self.device)\n",
        "        target_next = self.model(next_state).to(self.device)\n",
        "        target_val = self.target_model(next_state).to(self.device)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            if done[i]:\n",
        "                target[i][action[i]] = reward[i]\n",
        "            else:\n",
        "                if self.ddqn:\n",
        "                    a = torch.argmax(target_next[i]).item()\n",
        "                    target[i][action[i]] = reward[i] + self.gamma * target_val[i][a]\n",
        "                else:\n",
        "                    target[i][action[i]] = reward[i] + self.gamma * torch.max(target_val[i]).item()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = nn.MSELoss()(self.model(state), target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_state_dict(torch.load(name))\n",
        "\n",
        "    def save(self, name):\n",
        "        torch.save(self.model.state_dict(), name)\n",
        "\n",
        "    def plot_model(self, score, episode):\n",
        "        self.scores.append(score)\n",
        "        self.episodes.append(episode)\n",
        "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
        "        plt.plot(self.episodes, self.average, 'r')\n",
        "        plt.plot(self.episodes, self.scores, 'b')\n",
        "        plt.ylabel('Score', fontsize=18)\n",
        "        plt.xlabel('Steps', fontsize=18)\n",
        "        dqn = 'DQN_'\n",
        "        softupdate = ''\n",
        "        dueling = ''\n",
        "        greedy = ''\n",
        "        if self.ddqn: dqn = 'DDQN_'\n",
        "        if self.Soft_Update: softupdate = '_soft'\n",
        "        if self.dueling: dueling = '_Dueling'\n",
        "        if self.epsilot_greedy: greedy = '_Greedy'\n",
        "        plt.savefig(dqn + self.env_name + softupdate + dueling + greedy + \".png\")\n",
        "        plt.close()\n",
        "        return str(self.average[-1])[:5]\n",
        "\n",
        "    def run(self):\n",
        "        decay_step = 0\n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.env.reset()\n",
        "            state = np.reshape(state, [1, self.state_size])\n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                decay_step += 1\n",
        "                action, explore_probability = self.act(state, decay_step)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                next_state = np.reshape(next_state, [1, self.state_size])\n",
        "                if not done or i == self.env._max_episode_steps-1:\n",
        "                    reward = reward\n",
        "                else:\n",
        "                    reward = -100\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                i += 1\n",
        "                if done:\n",
        "                    self.update_target_model()\n",
        "                    average = self.plot_model(i, e)\n",
        "                    print(f\"episode: {e}/{self.EPISODES}, score: {i}, e: {explore_probability:.2}, average: {average}\")\n",
        "                    if e == (self.EPISODES - 1):\n",
        "                        print(\"Saving trained model\")\n",
        "                        self.save(\"/content/D3QN.pt\")\n",
        "                        break\n",
        "                self.replay()\n",
        "        self.env.close()\n",
        "\n",
        "    def test(self):\n",
        "        self.load(\"/content/D3QN.pt\")\n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.env.reset()\n",
        "            state = np.reshape(state, [1, self.state_size])\n",
        "            state = torch.FloatTensor(state).to(self.device)  # Convert state to PyTorch tensor and move to device\n",
        "            done = False\n",
        "            i = 0\n",
        "            img = plt.imshow(self.env.render(mode='rgb_array'))\n",
        "            text_action = plt.text(0, -10, '', fontsize=12, color='red')\n",
        "            text_reward = plt.text(100, -10, '', fontsize=12, color='blue')\n",
        "            text_step= plt.text(300, -10, '', fontsize=12, color='green')\n",
        "            text_score = plt.text(500, -10, '', fontsize=12, color='blue')\n",
        "\n",
        "            while not done:\n",
        "                self.env.render()\n",
        "                img.set_data(self.env.render(mode='rgb_array'))  # Update the data\n",
        "                # display.display(plt.gcf())\n",
        "                # display.clear_output(wait=True)\n",
        "\n",
        "                action = np.argmax(self.model(state).detach().cpu().numpy())  # Detach the tensor and convert it back to numpy\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                text_action.set_text(f'Action: {action}')\n",
        "                text_reward.set_text(f'Reward: {reward}')\n",
        "                text_step.set_text(f'Step: {e}')\n",
        "                text_score.set_text(f'Score: {int}')\n",
        "\n",
        "                state = np.reshape(next_state, [1, self.state_size])\n",
        "                state = torch.FloatTensor(state).to(self.device)  # Convert next_state to PyTorch tensor and move to device\n",
        "                i += 1\n",
        "\n",
        "                display.display(plt.gcf())\n",
        "                display.clear_output(wait=True)\n",
        "                if done:\n",
        "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
        "                    break\n"
      ],
      "metadata": {
        "id": "O9Cy58C0ByyM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = D3QN('CartPole-v1')\n",
        "agent.run()\n"
      ],
      "metadata": {
        "id": "qJCMO1yZ_8SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.test()"
      ],
      "metadata": {
        "id": "QbDulKVRAC6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}