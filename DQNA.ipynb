{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wybRSjEO_6aY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pylab\n",
        "\n",
        "class DQNN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 64)\n",
        "        self.fc4 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        return self.fc4(x)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make('CartPole-v1', new_step_api=True)\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.EPISODES = 1000\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        self.gamma = 0.95    # discount rate 0.95\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.001\n",
        "        self.epsilon_decay = 0.999\n",
        "        self.batch_size = 64\n",
        "        self.train_start = 1000\n",
        "        self.scores, self.episodes, self.average = [], [], []\n",
        "\n",
        "        # create main model\n",
        "        self.model = DQNN(self.state_size, self.action_size)\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.RMSprop(self.model.parameters(), lr=0.00025, alpha=0.95, eps=0.01)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.train_start:\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        else:\n",
        "            state = torch.FloatTensor(state)\n",
        "            with torch.no_grad():\n",
        "                return torch.argmax(self.model(state)).item()\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.train_start:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
        "\n",
        "        state = np.zeros((self.batch_size, self.state_size))\n",
        "        next_state = np.zeros((self.batch_size, self.state_size))\n",
        "        action, reward, done = [], [], []\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            state[i] = minibatch[i][0]\n",
        "            action.append(minibatch[i][1])\n",
        "            reward.append(minibatch[i][2])\n",
        "            next_state[i] = minibatch[i][3]\n",
        "            done.append(minibatch[i][4])\n",
        "\n",
        "        state = torch.FloatTensor(state)\n",
        "        next_state = torch.FloatTensor(next_state)\n",
        "        reward = torch.FloatTensor(reward)\n",
        "        done = torch.FloatTensor(done)\n",
        "        action = torch.LongTensor(action)\n",
        "\n",
        "        target = self.model(state)\n",
        "        target_next = self.model(next_state)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            if done[i]:\n",
        "                target[i][action[i]] = reward[i]\n",
        "            else:\n",
        "                target[i][action[i]] = reward[i] + self.gamma * torch.max(target_next[i])\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.criterion(self.model(state), target)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def run(self):\n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.env.reset()\n",
        "            state = np.reshape(state, [1, self.state_size])\n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                self.env.render()\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, _, _ = self.env.step(action)\n",
        "                next_state = np.reshape(next_state, [1, self.state_size])\n",
        "                if not done or i == self.env._max_episode_steps - 1:\n",
        "                    reward = reward\n",
        "                else:\n",
        "                    reward = -100\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                i += 1\n",
        "                if done:\n",
        "\n",
        "                    average = self.PlotModel(i, e)\n",
        "                    print(\"episode: {}/{}, score: {}, e: {:.2}, average: {}\".format(e, self.EPISODES, i, self.epsilon, average))\n",
        "                    if i == 50:\n",
        "                        print(\"Saving trained model as cartpole-dqn.pth\")\n",
        "                        self.save(\"/content/cartpole-dqn.pth\")\n",
        "                        return\n",
        "                self.replay()\n",
        "\n",
        "    def test(self):\n",
        "        self.load(\"/content/cartpole-dqn.pth\")\n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.env.reset()\n",
        "            state = np.reshape(state, [1, self.state_size])\n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                self.env.render()\n",
        "                state = torch.FloatTensor(state)\n",
        "                action = torch.argmax(self.model(state)).item()\n",
        "                next_state, reward, done, _, _ = self.env.step(action)\n",
        "                state = np.reshape(next_state, [1, self.state_size])\n",
        "                i += 1\n",
        "                if done:\n",
        "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
        "                    break\n",
        "\n",
        "        self.env.reset()\n",
        "        img = plt.imshow(self.env.render(mode='rgb_array'))  # only call this once\n",
        "        for _ in range(4):\n",
        "            img.set_data(self.env.render(mode='rgb_array'))  # just update the data\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "            action = self.env.action_space.sample()\n",
        "            self.env.step(action)\n",
        "\n",
        "    def save(self, name):\n",
        "        torch.save(self.model.state_dict(), name)\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_state_dict(torch.load(name))\n",
        "        self.model.eval()\n",
        "\n",
        "    pylab.figure(figsize=(18, 9))\n",
        "    def PlotModel(self, score, episode):\n",
        "        self.scores.append(score)\n",
        "        self.episodes.append(episode)\n",
        "        self.average.append(sum(self.scores) / len(self.scores))\n",
        "        pylab.plot(self.episodes, self.average, 'r')\n",
        "        pylab.plot(self.episodes, self.scores, 'b')\n",
        "        pylab.ylabel('Score', fontsize=18)\n",
        "        pylab.xlabel('Steps', fontsize=18)\n",
        "\n",
        "        return str(self.average[-1])[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent()\n",
        "agent.run()\n"
      ],
      "metadata": {
        "id": "qJCMO1yZ_8SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.test()"
      ],
      "metadata": {
        "id": "QbDulKVRAC6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}