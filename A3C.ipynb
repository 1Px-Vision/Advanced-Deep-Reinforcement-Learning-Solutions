{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import threading\n",
        "from threading import Lock\n",
        "from matplotlib import pylab\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class OurModel(nn.Module):\n",
        "    def __init__(self, input_shape, action_space):\n",
        "        super(OurModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(np.prod(input_shape), 512)\n",
        "        self.fc2_action = nn.Linear(512, action_space)\n",
        "        self.fc2_value = nn.Linear(512, 1)\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.elu(self.fc1(x))\n",
        "        action = nn.Softmax(dim=-1)(self.fc2_action(x))\n",
        "        value = self.fc2_value(x)\n",
        "        return action, value\n",
        "\n",
        "class A3CAgent:\n",
        "    def __init__(self, env_name):\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(env_name)\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.EPISODES, self.episode, self.max_average = 1000, 0, -21.0\n",
        "        self.lock = Lock()\n",
        "        self.state_size = self.env.observation_space.shape\n",
        "\n",
        "        self.ROWS = 80\n",
        "        self.COLS = 80\n",
        "        self.REM_STEP = 4\n",
        "        self.lr = 0.000025\n",
        "\n",
        "        self.scores, self.episodes, self.average = [], [], []\n",
        "\n",
        "        self.Save_Path = 'Models'\n",
        "        if not os.path.exists(self.Save_Path):\n",
        "            os.makedirs(self.Save_Path)\n",
        "        self.path = '{}_A3C_{}'.format(self.env_name, self.lr)\n",
        "        self.Model_name = os.path.join(self.Save_Path, self.path)\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.Actor = OurModel(input_shape=self.state_size, action_space=self.action_size).to(self.device)\n",
        "        self.Critic = OurModel(input_shape=self.state_size, action_space=1).to(self.device)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.Actor.parameters(), lr=self.lr)\n",
        "        self.critic_optimizer = optim.Adam(self.Critic.parameters(), lr=self.lr)\n",
        "\n",
        "    def discount_rewards(self, rewards):\n",
        "        gamma = 0.99\n",
        "        discounted_r = np.zeros_like(rewards)\n",
        "        R = 0\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            if rewards[i] != 0:\n",
        "                R = 0\n",
        "            R = R * gamma + rewards[i]\n",
        "            discounted_r[i] = R\n",
        "\n",
        "        discounted_r = torch.tensor(discounted_r, dtype=torch.float32).to(self.device)\n",
        "        discounted_r = (discounted_r - discounted_r.mean()) / (discounted_r.std() + 1e-5)\n",
        "        return discounted_r\n",
        "\n",
        "    def replay(self, states, actions, rewards):\n",
        "        states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in states]).to(self.device)\n",
        "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
        "        rewards = self.discount_rewards(rewards)\n",
        "\n",
        "        action_probs, values = self.Actor(states)\n",
        "        values = values.squeeze()\n",
        "        advantages = rewards - values.detach()\n",
        "\n",
        "        dist = Categorical(action_probs)\n",
        "        action_log_probs = dist.log_prob(actions)\n",
        "        actor_loss = -(action_log_probs * advantages).mean()\n",
        "        critic_loss = F.mse_loss(values, rewards)\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward(retain_graph=True)\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward(retain_graph=True)\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            action_probs, _ = self.Actor(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.item()\n",
        "\n",
        "    def load(self, actor_path, critic_path):\n",
        "        self.Actor.load_state_dict(torch.load(actor_path))\n",
        "        self.Critic.load_state_dict(torch.load(critic_path))\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.Actor.state_dict(), '/content/A3C_Actor.pt')\n",
        "        torch.save(self.Critic.state_dict(), '/content/A3C_Critic.pt')\n",
        "\n",
        "    pylab.figure(figsize=(18, 9))\n",
        "    def PlotModel(self, score, episode):\n",
        "        self.scores.append(score)\n",
        "        self.episodes.append(episode)\n",
        "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
        "        pylab.plot(self.episodes, self.average, 'r')\n",
        "        pylab.plot(self.episodes, self.scores, 'b')\n",
        "        pylab.ylabel('Score', fontsize=18)\n",
        "        pylab.xlabel('Steps', fontsize=18)\n",
        "        try:\n",
        "            pylab.savefig(self.path + \".png\")\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "        return float(self.average[-1])  # Convert average to float\n",
        "\n",
        "    def run(self):\n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "            states, actions, rewards = [], [], []\n",
        "            while not done:\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                states.append(state)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                state = next_state\n",
        "                score += reward\n",
        "            self.replay(states, actions, rewards)\n",
        "            print(f\"Episode {e+1}/{self.EPISODES}: Score = {score}\")\n",
        "        self.env.close()\n",
        "\n",
        "    def train(self, n_threads):\n",
        "        self.env.close()\n",
        "        envs = [gym.make(self.env.unwrapped.spec.id) for _ in range(n_threads)]\n",
        "        threads = [\n",
        "            threading.Thread(target=self.train_threading, daemon=True, args=(self, envs[i], i))\n",
        "            for i in range(n_threads)\n",
        "        ]\n",
        "        for t in threads:\n",
        "            time.sleep(2)\n",
        "            t.start()\n",
        "\n",
        "    def train_threading(self, agent, env, thread):\n",
        "        while self.episode < self.EPISODES:\n",
        "            score, done, SAVING = 0, False, ''\n",
        "            state = env.reset()\n",
        "            states, actions, rewards = [], [], []\n",
        "            while not done:\n",
        "                action = agent.act(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                states.append(state)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                score += reward\n",
        "                state = next_state\n",
        "\n",
        "            self.lock.acquire()\n",
        "            self.replay(states, actions, rewards)\n",
        "            self.lock.release()\n",
        "\n",
        "            with self.lock:\n",
        "                average = self.PlotModel(score, self.episode)\n",
        "                if average >= self.max_average:\n",
        "                    self.max_average = average\n",
        "                    self.save()\n",
        "                    SAVING = \"SAVING\"\n",
        "                else:\n",
        "                    SAVING = \"\"\n",
        "                print(f\"episode: {self.episode}/{self.EPISODES}, thread: {thread}, score: {score}, average: {average:.2f} {SAVING}\")\n",
        "                if self.episode < self.EPISODES:\n",
        "                    self.episode += 1\n",
        "        env.close()\n",
        "\n",
        "\n",
        "    def test(self, Actor_name, Critic_name):\n",
        "        self.load(Actor_name, Critic_name)\n",
        "        self.Actor.eval()\n",
        "        self.Critic.eval()\n",
        "\n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.env.reset()\n",
        "            state = np.zeros(self.state_size)\n",
        "            state = torch.FloatTensor(state).to(self.device)\n",
        "            done = False\n",
        "            i = 0\n",
        "            img = plt.imshow(self.env.render(mode='rgb_array'))\n",
        "            text_action = plt.text(0, -10, '', fontsize=12, color='red')\n",
        "            text_reward = plt.text(100, -10, '', fontsize=12, color='blue')\n",
        "            text_step = plt.text(300, -10, '', fontsize=12, color='green')\n",
        "            text_score = plt.text(500, -10, '', fontsize=12, color='blue')\n",
        "\n",
        "            while not done:\n",
        "                img.set_data(self.env.render(mode='rgb_array'))\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    action_probs = self.Actor(state.unsqueeze(0))\n",
        "                    action = torch.argmax(action_probs[0], dim=1).item()\n",
        "\n",
        "                next_state, reward, done, info = self.env.step(action)\n",
        "                next_state = torch.FloatTensor(next_state).to(self.device)\n",
        "\n",
        "                text_action.set_text(f'Action: {action}')\n",
        "                text_reward.set_text(f'Reward: {reward}')\n",
        "                text_step.set_text(f'Step: {i}')\n",
        "                text_score.set_text(f'Score: {e}')\n",
        "\n",
        "                state = next_state\n",
        "                i += 1\n",
        "\n",
        "                plt.pause(0.01)\n",
        "                plt.draw()\n",
        "\n",
        "                if done:\n",
        "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
        "                    break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hhYqhLapE9Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = A3CAgent('CartPole-v1')\n",
        "agent.run()\n"
      ],
      "metadata": {
        "id": "qJCMO1yZ_8SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.test('/content/A3C_Actor.pt','/content/A3C_Critic.pt')"
      ],
      "metadata": {
        "id": "QbDulKVRAC6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}