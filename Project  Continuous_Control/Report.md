# Project 2: Continuous Control

## Description of the implementation

I have explored and implemented the Deep Deterministic Policy Gradient (DDPG) Algorithm to address this challenge.

## Development

I started this project by implementing the DDPG Algorithm, which combines the actor-critic approach with insights from the recent success of Deep Q-Network (DQN). After thoroughly reviewing the original DDPG paper, I was confident that this method could yield promising results. Thus, I accepted the challenge of solving this environment using only the principles outlined in this model. First, I designed the Actor and Critic networks to be flexible, allowing for customization of the number of hidden layers and nodes in each layer. This approach enabled easy tuning of their depth and size. Next, I developed the Agent with the initial objective of simply instantiating the networks and taking actions, which allowed me to test whether the Actor was functioning correctly. I then implemented the replay buffer to store experiences and facilitate the training phase. Subsequently, I wrote the learning function along with the soft-update method. With this step completed, I was able to verify the correctness of the entire training process. Before diving into the final phase, I incorporated an exploration noise process—the Ornstein-Uhlenbeck process—along with other minor adjustments, such as initializing the networks' weights. Finally, it was time to fine-tune the hyperparameters to solve the environment in as few episodes as possible. This phase was conducted using version 2, which operates with 20 agents simultaneously. I provide a more detailed account of this phase in the section 'Fine-tuning the Hyperparameters.

## Network architecture

The actor-network takes 33 variables representing the observation space as input and generates 4 numbers as output, representing the predicted optimal action for the observed state. In essence, the Actor approximates the optimal policy π deterministically. The Critic Network also receives 33 variables representing the observation space as input. The output from the Critic's first hidden layer is combined with the action generated by the Actor Network, and this combined data is fed into the Critic's second hidden layer. The Critic then outputs a prediction of the target value based on the current state and the estimated optimal action. In other words, the Critic computes the optimal action-value function Q(s, a) by leveraging the action the Actor deems most appropriate.

## Fine-tuning the hyperparameters

I experimented with various configurations for the network's depth and size, such as [128, 128], [128, 32], [64, 64, 64], and [200, 200]. However, I ultimately adopted the structure recommended in the paper: two hidden layers with 400 nodes in the first layer and 300 nodes in the second layer for both the Actor and Critic networks. The paper suggests using minibatches of size 64, with learning rates of 10^-4 for the Actor and 10^-3 for the Critic. After testing these and other configurations, I used minibatches of size 128 and a learning rate of 10^-3 for both networks. Regarding the replay buffer size, after some experimentation, I settled on 10^5, which is sufficient to store experiences from 5 full episodes of 1,000 steps each, considering using version 2 with 20 agents. The optimal value found for the discount factor (gamma) was 0.99, prioritizing rewards approximately 100 steps ahead. Soft updates were applied at a rate of 10^-3 per learning step, with the target networks' weights being completely replaced after approximately every episode. For the exploration noise process, I determined a decay rate of 0.999, effectively minimizing the noise impact due to how this rate was updated.

In summary, the following values were assigned to the hyperparameters:

* fc_layers for the actor-network: FC1: 400 nodes, FC2: 300 nodes
* fc_layers for the critic network: FC1: 400 nodes, FC2: 300 nodes
* max_t: 1,000
* buffer_size: 10^5
* batch_size: 128
* lr_actor: 10^-3
* lr_critic: 10^-3
* gamma: 0.99
* tau: 10^-3
* noise_decay: 0.999
* target_score: 30.0
* target_episodes: 50

## Result
### Plot of the rewards

This graph presents the rewards per episode for all agents during the training phase and the moving average. It demonstrates that the Agent achieved a moving average reward of at least 30.0 points over the initial 100 episodes.

## Trained Agent

### Untrained Agent

![Untrained_Agent](https://github.com/1Px-Vision/Advanced-Deep-Reinforcement-Learning-Solutions/blob/main/Project%20%20Continuous_Control/untrained_agent.gif)

### Trained Agent

![Trained_Agent](https://github.com/1Px-Vision/Advanced-Deep-Reinforcement-Learning-Solutions/blob/main/Project%20%20Continuous_Control/trained_agent.gif)

## Future Work

* While I have invested significant time in fine-tuning the hyperparameters, alternative configurations that enable the Agent to solve the environment more efficiently may still exist. Therefore, additional testing could be conducted to explore these possibilities.
* Given that the current project settings effectively nullified the exploration noise process, further experiments might be performed by introducing noise during the training phase.
* The DDPG paper employs uniformly sampled minibatches from the replay buffer. As an alternative, implementing a prioritized replay buffer could provide valuable insights for comparison.
* Introducing negative rewards could deter the Agent from making random moves that diverge from its primary objective of maintaining contact with the target.
* This project aimed to achieve a reward of +30.0 over 100 consecutive episodes. Future tests might investigate whether this architecture can solve the environment with a higher target score.
* Additionally, other actor-critic Algorithms are designed to tackle this type of environment. Future work could explore their implementation to assess their performance. Some of those algorithms are:
   * TRPO - Trust Region Policy Optimization
   * GAE - Generalized Advantage Estimation
   *  A3C - Asynchronous Advantage Actor-Critic
   * ACER - Actor Critic with Experience Replay
   * PPO - Proximal Policy Optimization
   * D4PG - Distributed Distributional Deterministic Policy Gradients
  
  
