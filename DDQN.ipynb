{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import pylab\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import cv2\n",
        "from IPython import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import pygame\n",
        "\n",
        "class OurModel(nn.Module):\n",
        "    def __init__(self, input_shape, action_space):\n",
        "        super(OurModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_shape[0], 24)\n",
        "        self.fc2 = nn.Linear(24, 24)\n",
        "        self.fc3 = nn.Linear(24, action_space)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, env_name):\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(env_name)\n",
        "        #self.env.seed(0)\n",
        "        self.env._max_episode_steps = 4000\n",
        "        self.state_size = self.env.observation_space.shape[0]\n",
        "        self.action_size = self.env.action_space.n\n",
        "\n",
        "        self.EPISODES = 100\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.999\n",
        "        self.batch_size = 32\n",
        "        self.train_start = 1000\n",
        "\n",
        "        self.ROWS = 160\n",
        "        self.COLS = 240\n",
        "        self.REM_STEP=4\n",
        "        self.image_memory = np.zeros((self.REM_STEP, self.ROWS, self.COLS))\n",
        "\n",
        "\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((640, 480))\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.metadata = {\"render_fps\": 30}\n",
        "\n",
        "        self.ddqn = True\n",
        "        self.Soft_Update = False\n",
        "\n",
        "        self.TAU = 0.1\n",
        "\n",
        "        self.Save_Path = 'Models'\n",
        "        self.scores, self.episodes, self.average = [], [], []\n",
        "\n",
        "        if self.ddqn:\n",
        "            print(\"----------Double DQN--------\")\n",
        "            self.Model_name = os.path.join(self.Save_Path, \"DDQN_\"+self.env_name+\".pt\")\n",
        "        else:\n",
        "            print(\"-------------DQN------------\")\n",
        "            self.Model_name = os.path.join(self.Save_Path, \"DQN_\"+self.env_name+\".pt\")\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = OurModel(input_shape=(self.state_size,), action_space=self.action_size).float()\n",
        "        self.target_model = OurModel(input_shape=(self.state_size,), action_space=self.action_size).float()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def update_target_model(self):\n",
        "        if not self.Soft_Update and self.ddqn:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "        elif self.Soft_Update and self.ddqn:\n",
        "            for target_param, param in zip(self.target_model.parameters(), self.model.parameters()):\n",
        "                target_param.data.copy_(target_param.data * (1.0 - self.TAU) + param.data * self.TAU)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.train_start and self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        else:\n",
        "            state = torch.FloatTensor(state).float()\n",
        "            with torch.no_grad():\n",
        "                return np.argmax(self.model(state).numpy())\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.train_start:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
        "\n",
        "        state = np.zeros((self.batch_size, self.state_size))\n",
        "        next_state = np.zeros((self.batch_size, self.state_size))\n",
        "        action, reward, done = [], [], []\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            state[i] = minibatch[i][0]\n",
        "            action.append(minibatch[i][1])\n",
        "            reward.append(minibatch[i][2])\n",
        "            next_state[i] = minibatch[i][3]\n",
        "            done.append(minibatch[i][4])\n",
        "\n",
        "        state = torch.FloatTensor(state).float()\n",
        "        next_state = torch.FloatTensor(next_state).float()\n",
        "        action = torch.LongTensor(action).unsqueeze(1)\n",
        "        reward = torch.FloatTensor(reward)\n",
        "        done = torch.FloatTensor(done)\n",
        "\n",
        "        q_values = self.model(state).gather(1, action).squeeze(1)\n",
        "        next_q_values = self.model(next_state).max(1)[0]\n",
        "        next_q_state_values = self.target_model(next_state).max(1)[0]\n",
        "\n",
        "        target = reward + (1 - done) * self.gamma * next_q_state_values\n",
        "\n",
        "        if self.ddqn:\n",
        "            next_q_action = self.model(next_state).max(1)[1]\n",
        "            next_q_state_values = self.target_model(next_state).gather(1, next_q_action.unsqueeze(1)).squeeze(1)\n",
        "            target = reward + (1 - done) * self.gamma * next_q_state_values\n",
        "\n",
        "        loss = self.criterion(q_values, target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_state_dict(torch.load(name))\n",
        "\n",
        "    def save(self, name):\n",
        "        torch.save(self.model.state_dict(), name)\n",
        "\n",
        "    def step(self,action):\n",
        "        next_state, reward, done, info = self.env.step(action)\n",
        "        #next_state = self.GetImage()#action,next_state\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def reset(self,action,next_state):\n",
        "        self.env.reset()\n",
        "        for i in range(self.REM_STEP):\n",
        "            state = self.GetImage()#action,next_state\n",
        "        return state\n",
        "\n",
        "\n",
        "    def imshow(self, image, rem_step=0):\n",
        "        cv2_imshow(image[rem_step,...])\n",
        "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
        "            cv2.destroyAllWindows()\n",
        "            return\n",
        "\n",
        "    def GetImage(self):\n",
        "        img = self.env.render(mode='rgb_array')\n",
        "\n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        img_rgb_resized = cv2.resize(img_rgb, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
        "        img_rgb_resized[img_rgb_resized < 255] = 0\n",
        "        img_rgb_resized = img_rgb_resized / 255\n",
        "\n",
        "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
        "        self.image_memory[0,:,:] = img_rgb_resized\n",
        "\n",
        "        self.imshow(self.image_memory,0)\n",
        "\n",
        "        return np.expand_dims(self.image_memory, axis=0)\n",
        "\n",
        "        #return np.expand_dims(self.image_memory, axis=0)\n",
        "\n",
        "    def PlotModel(self, score, episode):\n",
        "        self.scores.append(score)\n",
        "        self.episodes.append(episode)\n",
        "        self.average.append(sum(self.scores) / len(self.scores))\n",
        "        pylab.plot(self.episodes, self.average, 'r')\n",
        "        pylab.plot(self.episodes, self.scores, 'b')\n",
        "        pylab.ylabel('Score', fontsize=18)\n",
        "        pylab.xlabel('Steps', fontsize=18)\n",
        "        dqn = 'DQN_'\n",
        "        softupdate = ''\n",
        "        if self.ddqn:\n",
        "            dqn = 'DDQN_'\n",
        "        if self.Soft_Update:\n",
        "            softupdate = '_soft'\n",
        "        try:\n",
        "            pylab.savefig(dqn+self.env_name+softupdate+\".png\")\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "        return str(self.average[-1])[:5]\n",
        "\n",
        "    def run(self):\n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.env.reset()\n",
        "            #state = self.reset()\n",
        "            state = np.reshape(state, [1, self.state_size])\n",
        "            done = False\n",
        "            i = 0\n",
        "            while not done:\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done,info = self.step(action)#self.env\n",
        "                next_state = np.reshape(next_state, [1, self.state_size])\n",
        "                reward = reward if not done or i == self.env._max_episode_steps-1 else -100\n",
        "                self.remember(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                i += 1\n",
        "                if done:\n",
        "                    self.update_target_model()\n",
        "                    average = self.PlotModel(i, e)\n",
        "                    print(\"episode: {}/{}, score: {}, e: {:.2}, average: {}\".format(e, self.EPISODES, i, self.epsilon, average))\n",
        "                    if e ==(self.EPISODES-1) :#(self.env._max_episode_steps-1):\n",
        "                        print(\"Saving trained model as cartpole-ddqn.pt\")\n",
        "                        self.save('/content/cartpole-ddqn.pt')\n",
        "                        #break\n",
        "                self.replay()\n",
        "\n",
        "    def test(self):\n",
        "        self.load(\"/content/cartpole-ddqn.pt\")\n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.env.reset()\n",
        "            state = np.reshape(state, [1, self.state_size])\n",
        "            state = torch.FloatTensor(state).to(self.device)  # Convert state to PyTorch tensor\n",
        "            done = False\n",
        "            i = 0\n",
        "            img = plt.imshow(self.env.render(mode='rgb_array'))\n",
        "            text_action = plt.text(0, -10, '', fontsize=12, color='red')\n",
        "            text_reward = plt.text(100, -10, '', fontsize=12, color='blue')\n",
        "            text_step= plt.text(300, -10, '', fontsize=12, color='green')\n",
        "            text_score = plt.text(500, -10, '', fontsize=12, color='blue')\n",
        "\n",
        "            while not done:\n",
        "                self.env.render()\n",
        "                img.set_data(self.env.render(mode='rgb_array'))  # Update the data\n",
        "                #display.display(plt.gcf())\n",
        "                #display.clear_output(wait=True)\n",
        "\n",
        "                action = np.argmax(self.model(state).detach().numpy())  # Detach the tensor and convert it back to numpy\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                text_action.set_text(f'Action: {action}')\n",
        "                text_reward.set_text(f'Reward: {reward}')\n",
        "                text_step.set_text(f'Step: {i}')\n",
        "                text_score.set_text(f'Score: {e}')\n",
        "\n",
        "                state = np.reshape(next_state, [1, self.state_size])\n",
        "                state = torch.FloatTensor(state)  # Convert next_state to PyTorch tensor\n",
        "                i += 1\n",
        "\n",
        "                display.display(plt.gcf())\n",
        "                display.clear_output(wait=True)\n",
        "                if done:\n",
        "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
        "                    break\n"
      ],
      "metadata": {
        "id": "LLpE8VEQAsuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQNAgent('CartPole-v1')\n",
        "agent.run()\n"
      ],
      "metadata": {
        "id": "qJCMO1yZ_8SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.test()"
      ],
      "metadata": {
        "id": "QbDulKVRAC6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}