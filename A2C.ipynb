{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gym\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pygame\n",
        "from IPython import display\n",
        "\n",
        "class OurModel(nn.Module):\n",
        "    def __init__(self, input_shape, action_space):\n",
        "        super(OurModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(np.prod(input_shape), 512)\n",
        "        self.fc2_action = nn.Linear(512, action_space)\n",
        "        self.fc2_value = nn.Linear(512, 1)\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.elu(self.fc1(x))\n",
        "        action = nn.Softmax(dim=-1)(self.fc2_action(x))\n",
        "        value = self.fc2_value(x)\n",
        "        return action, value\n",
        "\n",
        "class A2CAgent:\n",
        "    def __init__(self, env_name):\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(env_name)\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.EPISODES, self.max_average = 1000, -21.0\n",
        "        self.lr = 0.000025\n",
        "        self.state_size = self.env.observation_space.shape\n",
        "\n",
        "        self.ROWS = 80\n",
        "        self.COLS = 80\n",
        "        self.REM_STEP = 4\n",
        "\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((640, 480))\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.metadata = {\"render_fps\": 30}\n",
        "\n",
        "        self.states, self.actions, self.rewards = [], [], []\n",
        "        self.scores, self.episodes, self.average = [], [], []\n",
        "\n",
        "        self.Save_Path = 'Models'\n",
        "        self.image_memory = np.zeros(self.state_size)\n",
        "\n",
        "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
        "        self.path = '{}_A2C_{}'.format(self.env_name, self.lr)\n",
        "        self.Model_name = os.path.join(self.Save_Path, self.path)\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = OurModel(input_shape=self.state_size, action_space=self.action_size).to(self.device)\n",
        "        self.optimizer = optim.RMSprop(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "    def remember(self, state, action, reward):\n",
        "        self.states.append(state)\n",
        "        action_onehot = np.zeros([self.action_size])\n",
        "        action_onehot[action] = 1\n",
        "        self.actions.append(action_onehot)\n",
        "        self.rewards.append(reward)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        state = state.unsqueeze(0).to(self.device)  # Add batch dimension\n",
        "        action_probs, _ = self.model(state)\n",
        "        action_probs = action_probs.cpu().detach().numpy()[0]\n",
        "\n",
        "        # Check for NaNs\n",
        "        if np.any(np.isnan(action_probs)):\n",
        "            print(\"NaNs detected in action_probs:\", action_probs)\n",
        "            action_probs = np.nan_to_num(action_probs, nan=1.0 / self.action_size)  # Replace NaNs with equal probabilities\n",
        "\n",
        "        # Ensure probabilities sum to 1\n",
        "        action_probs = action_probs / np.sum(action_probs)\n",
        "\n",
        "        # Clip the probabilities to ensure valid range\n",
        "        action_probs = np.clip(action_probs, 1e-10, 1.0)\n",
        "        action_probs = action_probs / np.sum(action_probs)  # Normalize again after clipping\n",
        "\n",
        "        action = np.random.choice(self.action_size, p=action_probs)\n",
        "        return action\n",
        "\n",
        "    def discount_rewards(self, reward):\n",
        "        gamma = 0.99\n",
        "        running_add = 0\n",
        "        discounted_r = np.zeros_like(reward)\n",
        "        for i in reversed(range(len(reward))):\n",
        "            if reward[i] != 0:\n",
        "                running_add = 0\n",
        "            running_add = running_add * gamma + reward[i]\n",
        "            discounted_r[i] = running_add\n",
        "\n",
        "        discounted_r -= np.mean(discounted_r)\n",
        "        discounted_r /= np.std(discounted_r)\n",
        "        return discounted_r\n",
        "\n",
        "    def replay(self):\n",
        "        states = torch.FloatTensor(np.vstack(self.states)).to(self.device)\n",
        "        actions = torch.FloatTensor(np.vstack(self.actions)).to(self.device)\n",
        "        discounted_r = torch.FloatTensor(self.discount_rewards(self.rewards)).to(self.device)\n",
        "\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        action_probs, values = self.model(states)\n",
        "        values = values.squeeze()\n",
        "\n",
        "        advantages = discounted_r - values\n",
        "        critic_loss = advantages.pow(2).mean()\n",
        "        action_log_probs = torch.log(action_probs)\n",
        "        actor_loss = -(action_log_probs * actions).sum(dim=1) * advantages\n",
        "        actor_loss = actor_loss.mean()\n",
        "        loss = actor_loss + critic_loss\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.states, self.actions, self.rewards = [], [], []\n",
        "\n",
        "    def load(self, model_path):\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "        self.model.eval()\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.model.state_dict(),'/content/A2C_model.pt')\n",
        "\n",
        "    def plot_model(self, score, episode):\n",
        "        self.scores.append(score)\n",
        "        self.episodes.append(episode)\n",
        "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
        "        if str(episode)[-2:] == \"00\":\n",
        "            plt.plot(self.episodes, self.scores, 'b')\n",
        "            plt.plot(self.episodes, self.average, 'r')\n",
        "            plt.ylabel('Score', fontsize=18)\n",
        "            plt.xlabel('Steps', fontsize=18)\n",
        "            plt.savefig(self.path + \".png\")\n",
        "        return self.average[-1]\n",
        "\n",
        "    def run(self):\n",
        "        for e in range(self.EPISODES):\n",
        "            state = self.env.reset()\n",
        "            done, score, SAVING = False, 0, ''\n",
        "            while not done:\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                self.remember(state, action, reward)\n",
        "                state = next_state\n",
        "                score += reward\n",
        "                if done:\n",
        "                    average = self.plot_model(score, e)\n",
        "                    if e == (self.EPISODES - 1):\n",
        "                        print(\"Saving trained model\")\n",
        "                        self.save()\n",
        "                    else:\n",
        "                        SAVING = \"\"\n",
        "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(e, self.EPISODES, score, average, SAVING))\n",
        "                    self.replay()\n",
        "        self.env.close()\n",
        "\n",
        "    def test(self, model_path):\n",
        "     self.load(model_path)\n",
        "     for e in range(self.EPISODES):\n",
        "        state = self.env.reset()\n",
        "        state = np.reshape(state, (1, *self.state_size))  # Unpack the tuple correctly\n",
        "        state = torch.FloatTensor(state).to(self.device)  # Convert state to PyTorch tensor\n",
        "        done = False\n",
        "        i = 0\n",
        "        img = plt.imshow(self.env.render(mode='rgb_array'))\n",
        "        text_action = plt.text(0, -10, '', fontsize=12, color='red')\n",
        "        text_reward = plt.text(100, -10, '', fontsize=12, color='blue')\n",
        "        text_step = plt.text(300, -10, '', fontsize=12, color='green')\n",
        "        text_score = plt.text(500, -10, '', fontsize=12, color='blue')\n",
        "\n",
        "        while not done:\n",
        "            #self.env.render()\n",
        "            img.set_data(self.env.render(mode='rgb_array'))  # Update the data\n",
        "\n",
        "            action_probs, _ = self.model(state)\n",
        "            action = np.argmax(action_probs.cpu().detach().numpy())\n",
        "            next_state, reward, done, _ = self.env.step(action)\n",
        "            text_action.set_text(f'Action: {action}')\n",
        "            text_reward.set_text(f'Reward: {reward}')\n",
        "            text_step.set_text(f'Step: {i}')\n",
        "            text_score.set_text(f'Score: {e}')\n",
        "\n",
        "            state = np.reshape(next_state, (1, *self.state_size))  # Unpack the tuple correctly\n",
        "            state = torch.FloatTensor(state).to(self.device)  # Convert next_state to PyTorch tensor\n",
        "            i += 1\n",
        "\n",
        "            plt.pause(1)\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "\n",
        "            if done:\n",
        "                print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n",
        "                break\n"
      ],
      "metadata": {
        "id": "sR7fkd0YEnVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = A2CAgent('CartPole-v1')\n",
        "agent.run()\n"
      ],
      "metadata": {
        "id": "qJCMO1yZ_8SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.test()"
      ],
      "metadata": {
        "id": "QbDulKVRAC6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}